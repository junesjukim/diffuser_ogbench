diff --git a/diffuser/scripts/ogbench_train_value.py b/diffuser/scripts/ogbench_train_value.py
index d10dcd4..3715b4b 100755
--- a/diffuser/scripts/ogbench_train_value.py
+++ b/diffuser/scripts/ogbench_train_value.py
@@ -7,8 +7,11 @@ parent_dir = os.path.dirname(current_dir)
 sys.path.insert(0, parent_dir)
 
 import diffuser.utils as utils
+import torch
 from diffuser.models.diffusion import set_model_mode
 from diffuser.datasets.ogbench import OGBenchGoalDataset
+from diffuser.datasets.d4rl import load_environment
+from iql_pytorch.critic import Critic, ValueCritic
 
 class Parser(utils.Parser):
     dataset: str = 'scene-play-singletask-task3-v0'
@@ -20,6 +23,8 @@ class Parser(utils.Parser):
     normalizer: str = 'LimitsNormalizer'
     use_padding: bool = True
     max_path_length: int = 1000
+    q_path: str = '/home/junseolee/intern/diffuser_ogbench/iql_pytorch/runs/ogbench_runs/task3/test-task3-06-08-16-31-bs256-s0-t3.0-e0.7/model/critic_s700000.pth'
+    v_path: str = '/home/junseolee/intern/diffuser_ogbench/iql_pytorch/runs/ogbench_runs/task3/test-task3-06-08-16-31-bs256-s0-t3.0-e0.7/model/value_s700000.pth'
 
 args = Parser().parse_args('values')
 set_model_mode(args.prefix)
@@ -28,6 +33,23 @@ set_model_mode(args.prefix)
 #---------------------------------- dataset ----------------------------------#
 #-----------------------------------------------------------------------------#
 
+# 환경 정보 가져오기
+env = load_environment(args.dataset)
+state_dim = env.observation_space.shape[0]
+action_dim = env.action_space.shape[0]
+
+# Q와 V network 로드
+q_network = Critic(state_dim, action_dim).to(args.device)
+v_network = ValueCritic(state_dim, 256, 3).to(args.device)
+
+# state_dict 로드
+q_network.load_state_dict(torch.load(args.q_path, map_location=args.device))
+v_network.load_state_dict(torch.load(args.v_path, map_location=args.device))
+
+# 평가 모드로 설정
+q_network.eval()
+v_network.eval()
+
 dataset_config = utils.Config(
     OGBenchGoalDataset,
     savepath=(args.savepath, 'dataset_config.pkl'),
@@ -37,6 +59,8 @@ dataset_config = utils.Config(
     preprocess_fns=args.preprocess_fns,
     use_padding=args.use_padding,
     max_path_length=args.max_path_length,
+    q_network=q_network,
+    v_network=v_network,
 )
 
 dataset = dataset_config()
diff --git a/iql_pytorch/actor.py b/iql_pytorch/actor.py
deleted file mode 100644
index d63b5ae..0000000
--- a/iql_pytorch/actor.py
+++ /dev/null
@@ -1,32 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.distributions
-from common import MLP
-
-
-class Actor(nn.Module):
-    """MLP actor network."""
-
-    def __init__(
-        self, state_dim, action_dim, hidden_dim, n_layers, dropout_rate=None,
-        log_std_min=-10.0, log_std_max=2.0,
-    ):
-        super().__init__()
-
-        self.mlp = MLP(
-            state_dim, 2 * action_dim, hidden_dim, n_layers, dropout_rate=dropout_rate
-        )
-
-        self.log_std_min = log_std_min
-        self.log_std_max = log_std_max
-
-    def forward(
-        self, states
-    ):
-        mu, log_std = self.mlp(states).chunk(2, dim=-1)
-        mu = torch.tanh(mu)
-        return mu
-
-    def get_action(self, states):
-        mu = self.forward(states)
-        return mu
diff --git a/iql_pytorch/common.py b/iql_pytorch/common.py
deleted file mode 100644
index 10943b9..0000000
--- a/iql_pytorch/common.py
+++ /dev/null
@@ -1,41 +0,0 @@
-import torch.nn as nn
-from typing import Callable, Optional
-
-from torch.nn.modules.dropout import Dropout
-
-
-class MLP(nn.Module):
-
-    def __init__(
-        self,
-        in_dim,
-        out_dim,
-        hidden_dim,
-        n_layers,
-        activations: Callable = nn.ReLU,
-        activate_final: int = False,
-        dropout_rate: Optional[float] = None
-    ) -> None:
-        super().__init__()
-
-        self.affines = []
-        self.affines.append(nn.Linear(in_dim, hidden_dim))
-        for i in range(n_layers-2):
-            self.affines.append(nn.Linear(hidden_dim, hidden_dim))
-        self.affines.append(nn.Linear(hidden_dim, out_dim))
-        self.affines = nn.ModuleList(self.affines)
-
-        self.activations = activations()
-        self.activate_final = activate_final
-        self.dropout_rate = dropout_rate
-        if dropout_rate is not None:
-            self.dropout = Dropout(self.dropout_rate)
-
-    def forward(self, x):
-        for i in range(len(self.affines)):
-            x = self.affines[i](x)
-            if i != len(self.affines)-1 or self.activate_final:
-                x = self.activations(x)
-                if self.dropout_rate is not None:
-                    x = self.dropout(x)
-        return x
diff --git a/iql_pytorch/critic.py b/iql_pytorch/critic.py
deleted file mode 100644
index 14d9821..0000000
--- a/iql_pytorch/critic.py
+++ /dev/null
@@ -1,59 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-from common import MLP
-
-
-class ValueCritic(nn.Module):
-    def __init__(
-        self,
-        in_dim,
-        hidden_dim,
-        n_layers,
-        **kwargs
-    ) -> None:
-        super().__init__()
-        self.mlp = MLP(in_dim, 1, hidden_dim, n_layers, **kwargs)
-
-    def forward(self, state):
-        return self.mlp(state)
-
-
-class Critic(nn.Module):
-    """
-    From TD3+BC
-    """
-
-    def __init__(self, state_dim, action_dim):
-        super(Critic, self).__init__()
-
-        # Q1 architecture
-        self.l1 = nn.Linear(state_dim + action_dim, 256)
-        self.l2 = nn.Linear(256, 256)
-        self.l3 = nn.Linear(256, 1)
-
-        # Q2 architecture
-        self.l4 = nn.Linear(state_dim + action_dim, 256)
-        self.l5 = nn.Linear(256, 256)
-        self.l6 = nn.Linear(256, 1)
-
-    def forward(self, state, action):
-        sa = torch.cat([state, action], 1)
-
-        q1 = F.relu(self.l1(sa))
-        q1 = F.relu(self.l2(q1))
-        q1 = self.l3(q1)
-
-        q2 = F.relu(self.l4(sa))
-        q2 = F.relu(self.l5(q2))
-        q2 = self.l6(q2)
-        return q1, q2
-
-    def Q1(self, state, action):
-        sa = torch.cat([state, action], 1)
-
-        q1 = F.relu(self.l1(sa))
-        q1 = F.relu(self.l2(q1))
-        q1 = self.l3(q1)
-        return q1